{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAjhCtWgyk9MFqVn4rQR0n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishit605/AI-Art-generation/blob/main/Spactial_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Spactial Transformers** for CNNs and internalized augmented data adaptability\n"
      ],
      "metadata": {
        "id": "QDy52VfyQAV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# License: BSD\n",
        "# Author: Ghassen Hamrouni\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "plt.ion()   # interactive mode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiWZ3D9CQDTf",
        "outputId": "a83815a6-0e1f-48e1-e76e-fed0b381cc70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<contextlib.ExitStack at 0x7fce0d063820>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from six.moves import urllib\n",
        "opener = urllib.request.build_opener()\n",
        "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
        "urllib.request.install_opener(opener)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "! pip install torchinfo\n",
        "from torchinfo import summary"
      ],
      "metadata": {
        "id": "vFfc0llyNHtg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d366f0a-0313-43d0-a471-927457f7d121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir dataset\n",
        "%cd dataset\n",
        "\n",
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"BE0ifvf11oGilgCsqZFj\")\n",
        "project = rf.workspace(\"dylog\").project(\"birds-detections\")\n",
        "dataset = project.version(4).download(\"yolov8\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQd-7B9HiB6L",
        "outputId": "7e2b2a22-a39f-4ace-fb50-36a70fcc1f5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/dataset/dataset\n",
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: certifi==2022.12.7 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2022.12.7)\n",
            "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.0.0)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.10.0)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.10)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.22.4)\n",
            "Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.7.0.72)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (8.4.0)\n",
            "Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: supervision in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.12.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.26.16)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.2)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.65.0)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.1)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.1.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.41.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (23.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (2.0.12)\n",
            "Requirement already satisfied: opencv-python-headless<5.0.0.0,>=4.8.0.74 in /usr/local/lib/python3.10/dist-packages (from supervision->roboflow) (4.8.0.74)\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "[WARNING] we noticed you are downloading a `yolov8` datasets but you don't have `ultralytics` installed. Roboflow `.deploy` supports only models trained with `ultralytics==8.0.134`, to intall it `pip install ultralytics==8.0.134`.\n",
            "Downloading Dataset Version Zip in birds-detections-4 to yolov8: 100% [49791276 / 49791276] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to birds-detections-4 in yolov8:: 100%|██████████| 2974/2974 [00:00<00:00, 3706.19it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = '/content/dataset/dataset/birds-detections-4/train/images'\n",
        "valid_data = '/content/dataset/dataset/birds-detections-4/valid/images'"
      ],
      "metadata": {
        "id": "H0wb7xqrQSsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## THIS IS OBSOLETE NEWER VERSION IS BELOW ##\n",
        "\n",
        "class Img_fold(Dataset):\n",
        "  def __init__(self, tar_dir, transform=None):\n",
        "    self.paths = tar_dir\n",
        "    self.transform = transform\n",
        "\n",
        "  def load_image(self):\n",
        "    return Image.open(self.paths)\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    for files in os.listdir(self.paths):\n",
        "      item_path = os.path.join(self.paths, files)\n",
        "\n",
        "      path_list = []\n",
        "\n",
        "      if os.path.isfile(item_path) and item_path.endswith(('.jpg', '.png', '.jpeg')):\n",
        "          path_list.append(item_path)\n",
        "          return len(path_list)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image = self.load_image(self.paths)\n",
        "\n",
        "    if self.transform:\n",
        "      return self.transform(image)\n",
        "    else:\n",
        "      return image"
      ],
      "metadata": {
        "id": "ptKl5lbFSIX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs = cv2.imread('/content/dataset/Registration-Plate-3/train/images/frame227_jpg.rf.7034f4f9a4d92ec87716a7856cec1c15.jpg', 0)\n",
        "imgs = cv2.resize(imgs, (128,128))\n",
        "img_tensor = torch.from_numpy(imgs)\n",
        "type(img_tensor)"
      ],
      "metadata": {
        "id": "BAU1aX88tXBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, img_dir: str):\n",
        "    self.img_path = img_dir\n",
        "    file_list = glob.glob(self.img_path + \"*\")\n",
        "    # print(file_list)\n",
        "\n",
        "    self.data=[]\n",
        "    for class_path in file_list:\n",
        "      class_name = \"birds\"\n",
        "      for img_path in glob.glob(class_path + \"/*.jpg\"):\n",
        "        self.data.append([img_path, class_name])\n",
        "    # print(self.data)\n",
        "    self.class_map = {\"birds\" : 0}\n",
        "    self.img_dim = (128,128)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path, class_name = self.data[idx]\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.resize(img, self.img_dim)\n",
        "    # print(img.shape)\n",
        "\n",
        "    class_id = self.class_map[class_name]\n",
        "    img_tensor = torch.from_numpy(img)\n",
        "    img_tensor = img_tensor.permute(2, 0, 1)\n",
        "    class_id = torch.tensor([class_id],dtype=torch.float)\n",
        "    return img_tensor.float(), class_id.float()"
      ],
      "metadata": {
        "id": "3AX7JBTqiOGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_aug = transforms.Compose([\n",
        "    # transforms.Resize((128, 128)),\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "sczDL3PBQMLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8tF6zIffD0w",
        "outputId": "0d650824-bd3e-4c99-eb0e-1453667b8a95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 4\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "train_dataset = CustomDataset(train_data)\n",
        "\n",
        "# Training dataset\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           shuffle=True,\n",
        "                                           num_workers=NUM_WORKERS)\n",
        "\n",
        "# Validation Dataset\n",
        "valid_dataset = CustomDataset(valid_data)\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           shuffle=False,\n",
        "                                           num_workers=NUM_WORKERS)"
      ],
      "metadata": {
        "id": "wo5LCQw3Pusn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA-2rK9kcCmP",
        "outputId": "c6eb2591-8254-487f-f4fe-bda064c62194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7fcd332a18d0>"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for imgs, labels in train_loader:\n",
        "    print(\"Batch of images has shape: \",imgs.shape, type(imgs))\n",
        "    print(\"Batch of labels has shape: \", labels.shape, type(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WasXG4Tsp5Ob",
        "outputId": "5b91c45c-fde4-4aca-9b8d-68eea5fb713f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n",
            "Batch of images has shape:  torch.Size([4, 3, 128, 128]) <class 'torch.Tensor'>\n",
            "Batch of labels has shape:  torch.Size([4, 1]) <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)"
      ],
      "metadata": {
        "id": "3xarMjq4wDFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KoigM6qwEMq",
        "outputId": "aa1da4ec-2443-468d-fe97-73b492ca1841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 3, 128, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c1 = nn.Conv2d(3, 10, 5)\n",
        "p1 = nn.MaxPool2d(2, 2)\n",
        "d1 = nn.Dropout2d()\n",
        "c2 = nn.Conv2d(10, 20, 5)\n",
        "\n",
        "x = c1(images)\n",
        "x = p1(x)\n",
        "x = c2(x)\n",
        "x = p1(x)\n",
        "# x = d1(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6y3-02OuwEKJ",
        "outputId": "e25964a3-3481-47c4-e51f-092d08f948a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 20, 29, 29])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## THis is for GrayScale Images\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "        # Spatial transformer localization-network\n",
        "        self.localization = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, kernel_size=7),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(8, 10, kernel_size=5),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        # Regressor for the 3 * 2 affine matrix\n",
        "        self.fc_loc = nn.Sequential(\n",
        "            nn.Linear(10 * 3 * 3, 32),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(32, 3 * 2)\n",
        "        )\n",
        "\n",
        "        # Initialize the weights/bias with identity transformation\n",
        "        self.fc_loc[2].weight.data.zero_()\n",
        "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
        "\n",
        "    # Spatial transformer network forward function\n",
        "    def stn(self, x):\n",
        "        xs = self.localization(x)\n",
        "        xs = xs.view(-1, 10 * 3 * 3)\n",
        "        theta = self.fc_loc(xs)\n",
        "        theta = theta.view(-1, 2, 3)\n",
        "\n",
        "        grid = F.affine_grid(theta, x.size())\n",
        "        x = F.grid_sample(x, grid)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        # transform the input\n",
        "        x = self.stn(x)\n",
        "\n",
        "        # Perform the usual forward pass\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "model = Net().to(device)"
      ],
      "metadata": {
        "id": "6l6UvLqcgL4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model, input_size=(4, 3, 128, 128))"
      ],
      "metadata": {
        "id": "KAX85Og625a8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## For testing the Output Image shapes after Convolution\n",
        "c1 = nn.Conv2d(3, 32, 7)\n",
        "p1 = nn.MaxPool2d(2, 2)\n",
        "d1 = nn.ReLU()\n",
        "c2 = nn.Conv2d(32, 64, 5)\n",
        "\n",
        "x = c1(images)\n",
        "x = p1(x)\n",
        "x = d1(x)\n",
        "x = c2(x)\n",
        "x = p1(x)\n",
        "x = d1(x)\n",
        "x.shape"
      ],
      "metadata": {
        "id": "XPh42ivB4lLb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84987cd8-4e81-498e-d058-10e06136d6b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 64, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## This is for RGB images.\n",
        "\n",
        "class STN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(STN, self).__init__()\n",
        "        # simple convnet classifier\n",
        "        self.conv1 = nn.Conv2d(3, 16, 5)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(32*29*29, 128)\n",
        "        self.fc2 = nn.Linear(128, 84)\n",
        "        self.fc3 = nn.Linear(84,1)\n",
        "\n",
        "        # spatial transformer localization network\n",
        "        self.localization = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=7),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(32, 64, kernel_size=5),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        # tranformation regressor for theta\n",
        "        self.fc_loc = nn.Sequential(\n",
        "            nn.Linear(64*28*28, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 3 * 2)\n",
        "        )\n",
        "        # initializing the weights and biases with identity transformations\n",
        "        self.fc_loc[2].weight.data.zero_()\n",
        "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0],\n",
        "                                                    dtype=torch.float))\n",
        "    def stn(self, x):\n",
        "        xs = self.localization(x)\n",
        "        xs = xs.view(-1, xs.size(1)*xs.size(2)*xs.size(3))\n",
        "        # calculate the transformation parameters theta\n",
        "        theta = self.fc_loc(xs)\n",
        "        # resize theta\n",
        "        theta = theta.view(-1, 2, 3)\n",
        "        # grid generator => transformation on parameters theta\n",
        "        grid = F.affine_grid(theta, x.size())\n",
        "        # grid sampling => applying the spatial transformations\n",
        "        x = F.grid_sample(x, grid)\n",
        "        return x\n",
        "    def forward(self, x):\n",
        "        # transform the input\n",
        "        x = self.stn(x)\n",
        "\n",
        "        # forward pass through the classifier\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 32*29*29)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "model = STN().to(device)\n",
        "summary(model, input_size=(4, 3, 128, 128))"
      ],
      "metadata": {
        "id": "KWfPMSQc3ohV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd081810-d00f-4e04-bc51-53bf53730b45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "STN                                      [4, 1]                    --\n",
              "├─Sequential: 1-1                        [4, 64, 28, 28]           --\n",
              "│    └─Conv2d: 2-1                       [4, 32, 122, 122]         4,736\n",
              "│    └─MaxPool2d: 2-2                    [4, 32, 61, 61]           --\n",
              "│    └─ReLU: 2-3                         [4, 32, 61, 61]           --\n",
              "│    └─Conv2d: 2-4                       [4, 64, 57, 57]           51,264\n",
              "│    └─MaxPool2d: 2-5                    [4, 64, 28, 28]           --\n",
              "│    └─ReLU: 2-6                         [4, 64, 28, 28]           --\n",
              "├─Sequential: 1-2                        [4, 6]                    --\n",
              "│    └─Linear: 2-7                       [4, 256]                  12,845,312\n",
              "│    └─ReLU: 2-8                         [4, 256]                  --\n",
              "│    └─Linear: 2-9                       [4, 6]                    1,542\n",
              "├─Conv2d: 1-3                            [4, 16, 124, 124]         1,216\n",
              "├─MaxPool2d: 1-4                         [4, 16, 62, 62]           --\n",
              "├─Conv2d: 1-5                            [4, 32, 58, 58]           12,832\n",
              "├─MaxPool2d: 1-6                         [4, 32, 29, 29]           --\n",
              "├─Linear: 1-7                            [4, 128]                  3,444,864\n",
              "├─Linear: 1-8                            [4, 84]                   10,836\n",
              "├─Linear: 1-9                            [4, 1]                    85\n",
              "==========================================================================================\n",
              "Total params: 16,372,687\n",
              "Trainable params: 16,372,687\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 1.26\n",
              "==========================================================================================\n",
              "Input size (MB): 0.79\n",
              "Forward/backward pass size (MB): 33.23\n",
              "Params size (MB): 65.49\n",
              "Estimated Total Size (MB): 99.50\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_step(model, dataloader,\n",
        "              loss_fn, optimizer,\n",
        "              device=device):\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  train_loss, trian_acc = 0, 0\n",
        "\n",
        "  for batch, (images, labels)  in enumerate(train_loader):\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    y_pred = model(images)\n",
        "\n",
        "    # Convert labels to Long tensors\n",
        "    # labels = labels.long()\n",
        "\n",
        "    loss_fn = criterion(y_pred[0], labels)\n",
        "    train_loss += loss_fn.item()\n",
        "\n",
        "    loss_fn.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    _, preds = torch.max(y_pred.images, 1)\n",
        "    train_acc += (preds == labels).sum().item()/len(y_pred)\n",
        "\n",
        "  train_loss = train_loss/len(dataloader)\n",
        "  train_acc = train_acc/len(dataloader)\n",
        "  return train_loss, train_acc\n",
        "\n",
        "\n",
        "\n",
        "def test_step(model, dataloader,\n",
        "              loss_fn, optimizer,\n",
        "              device=device):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  test_loss, test_acc = 0, 0\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    for batch, (images, labels) in enumerate(valid_loader):\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "      y_test_pred = model(images)\n",
        "\n",
        "      loss_fn = criterion(y_test_pred[0], labels)\n",
        "      test_loss +=  loss_fn.item()\n",
        "\n",
        "      _, test_preds = torch.max(y_test_pred.images, 1)\n",
        "      test_acc += (test_preds == labels).sum().item()/len(y_test_pred)\n",
        "\n",
        "    test_loss = test_loss/len(dataloader)\n",
        "    test_acc = test_acc/len(dataloader)\n",
        "    return test_loss, test_acc\n",
        "\n",
        "\n",
        "# def train_step(model, dataloader,\n",
        "#               loss_fn, optimizer,\n",
        "#               device=device):\n",
        "\n",
        "#   model.train()\n",
        "\n",
        "#   train_loss, trian_acc = 0, 0\n",
        "\n",
        "#   for batch, (images, labels)  in enumerate(train_loader):\n",
        "#     images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "#     optimizer.zero_grad()\n",
        "\n",
        "#     y_pred = model(images)\n",
        "#     image, logits = y_pred\n",
        "\n",
        "  #   loss_fn = criterion(image, labels)\n",
        "  #   train_loss += loss_fn.item()\n",
        "\n",
        "  #   loss_fn.backward()\n",
        "\n",
        "  #   optimizer.step()\n",
        "\n",
        "  #   _, preds = torch.max(logits, 1)\n",
        "  #   train_acc += (preds == labels).sum().item()/len(y_pred)\n",
        "\n",
        "  # train_loss = train_loss/len(dataloader)\n",
        "  # train_acc = train_acc/len(dataloader)\n",
        "  # return train_loss, train_acc\n",
        "\n",
        "# def test_step(model, dataloader,\n",
        "#               loss_fn, optimizer,\n",
        "#               device=device):\n",
        "\n",
        "#   model.eval()\n",
        "\n",
        "#   test_loss, test_acc = 0, 0\n",
        "\n",
        "#   with torch.inference_mode():\n",
        "#     for batch, (images, labels) in enumerate(dataloader):\n",
        "#       images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "#       y_test_pred = model(images)\n",
        "#       image, logits = y_test_pred\n",
        "\n",
        "#       loss_fn = criterion(image, labels)\n",
        "#       test_loss += loss_fn.item()\n",
        "\n",
        "#       _, test_preds = torch.max(logits, 1)\n",
        "#       test_acc += (test_preds == labels).sum().item()/len(y_test_pred)\n",
        "\n",
        "#     test_loss = test_loss/len(dataloader)\n",
        "#     test_acc = test_acc/len(dataloader)\n",
        "#     return test_loss, test_acc\n"
      ],
      "metadata": {
        "id": "mVAv-s13Nzx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_to_numpy(image_grid, epoch):\n",
        "    \"\"\"\n",
        "    This function transforms the PyTorch image grids\n",
        "    into NumPy format that we will denormalize and save\n",
        "    as PNG file.\n",
        "    \"\"\"\n",
        "    image_grid = image_grid.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    image_grid = std * image_grid + mean\n",
        "    return image_grid"
      ],
      "metadata": {
        "id": "R03WJqAoZknH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "def stn_grid(epoch):\n",
        "    \"\"\"\n",
        "    This function will pass one batch of the test\n",
        "    image to the STN model and get the transformed images\n",
        "    after each epoch to save as PNG file and also as\n",
        "    GIFFY file.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        data = next(iter(valid_loader))[0].to(device)\n",
        "        transformed_image = model.stn(data).cpu().detach()\n",
        "        image_grid = torchvision.utils.make_grid(transformed_image)\n",
        "        # save the grid image\n",
        "        image_grid = transform_to_numpy(image_grid, epoch)\n",
        "        plt.imshow(image_grid)\n",
        "        plt.savefig(f\"../outputs/image_{epoch}.png\")\n",
        "        plt.close()\n",
        "        images.append(image_grid)"
      ],
      "metadata": {
        "id": "03fTb1D9Zkjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # train for certain epochs\n",
        "\n",
        "# epochs = 40\n",
        "\n",
        "# for epoch in range(epochs):\n",
        "#     print(f\"Epoch {epoch+1} of {epochs}\")\n",
        "\n",
        "    # train_loss, train_acc = train_step(model, train_loader,\n",
        "    #                                    optimizer, criterion)\n",
        "\n",
        "#     test_loss, test_acc = test_step(model, valid_loader,\n",
        "#                                     optimizer, criterion)\n",
        "#     print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}\")\n",
        "#     print(f\"Validation Loss: {test_loss:.4f}, Val Acc: {test_acc:.2f}\")\n",
        "#     stn_grid(epoch)"
      ],
      "metadata": {
        "id": "SgHdxF-MZkge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss, train_acc = train_step(model, train_loader,\n",
        "                                       optimizer, criterion)"
      ],
      "metadata": {
        "id": "GYmpcLeOZkTi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "0e195ba5-ce70-430d-ee80-9293c083061c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-029ed05b36d8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_loss, train_acc = train_step(model, train_loader,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                        optimizer, criterion)\n",
            "\u001b[0;32m<ipython-input-97-abd589fa954c>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, dataloader, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# labels = labels.long()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3029\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RGB Fin~\n",
        "\n",
        "---\n",
        "\n",
        "## GrayScale Cont-"
      ],
      "metadata": {
        "id": "OjAcQTvjZlh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from six.moves import urllib\n",
        "opener = urllib.request.build_opener()\n",
        "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
        "urllib.request.install_opener(opener)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training dataset\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST(root='.', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])), batch_size=64, shuffle=True, num_workers=4)\n",
        "# Test dataset\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST(root='.', train=False, transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])), batch_size=64, shuffle=True, num_workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpBJ1x0FdJ8z",
        "outputId": "f9e1a3f4-dfbe-4993-b205-2515d26cdebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 252270616.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 22407638.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 206007905.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 15822698.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 500 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "#\n",
        "# A simple test procedure to measure the STN performances on MNIST.\n",
        "#\n",
        "\n",
        "\n",
        "def test():\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "\n",
        "            # sum up batch loss\n",
        "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "            # get the index of the max log-probability\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
        "              .format(test_loss, correct, len(test_loader.dataset),\n",
        "                      100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "Ih2QRNr0gOAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_image_np(inp):\n",
        "    \"\"\"Convert a Tensor to numpy image.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    return inp\n",
        "\n",
        "# We want to visualize the output of the spatial transformers layer\n",
        "# after the training, we visualize a batch of input images and\n",
        "# the corresponding transformed batch using STN.\n",
        "\n",
        "\n",
        "def visualize_stn():\n",
        "    with torch.no_grad():\n",
        "        # Get a batch of training data\n",
        "        data = next(iter(test_loader))[0].to(device)\n",
        "\n",
        "        input_tensor = data.cpu()\n",
        "        transformed_input_tensor = model.stn(data).cpu()\n",
        "\n",
        "        in_grid = convert_image_np(\n",
        "            torchvision.utils.make_grid(input_tensor))\n",
        "\n",
        "        out_grid = convert_image_np(\n",
        "            torchvision.utils.make_grid(transformed_input_tensor))\n",
        "\n",
        "        # Plot the results side-by-side\n",
        "        f, axarr = plt.subplots(1, 2)\n",
        "        axarr[0].imshow(in_grid)\n",
        "        axarr[0].set_title('Dataset Images')\n",
        "\n",
        "        axarr[1].imshow(out_grid)\n",
        "        axarr[1].set_title('Transformed Images')\n",
        "\n",
        "for epoch in range(1, 5 + 1):\n",
        "    train(epoch)\n",
        "    test()\n",
        "\n",
        "# Visualize the STN transformation on some input batch\n",
        "visualize_stn()\n",
        "\n",
        "plt.ioff()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "scZwNXD1gPPq",
        "outputId": "b72758b1-3459-4b34-ae09-cb8c2a836343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/1024 (0%)]\tLoss: -0.000000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-30b6e780ede3>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-540a1ce64677>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-4a07d8121bcb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# transform the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# forward pass through the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-4a07d8121bcb>\u001b[0m in \u001b[0;36mstn\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m                                                     dtype=torch.float))\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# calculate the transformation parameters theta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[0m\u001b[1;32m    167\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                             return_indices=self.return_indices)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YRixPSl-gRLi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}